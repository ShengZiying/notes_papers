# The Revolution of Multimodal Large Language Models: A Survey

**Author**: Davide Caffagni1*, Federico Cocchi1,2*, Luca Barsellotti1*, Nicholas Moratelli1*, Sara Sarto1*, Lorenzo Baraldi2*, Lorenzo Baraldi1, Marcella Cornia1, and Rita Cucchiara1,3

**Publish Date**: 2024

**Add Date**: 2025.10.15

**Journal/Meeting**: Findings of the Association for Computational Linguistics ACL 2024

**Star**: ğŸŒŸğŸŒŸğŸŒŸğŸŒŸ

**PDF**: [The Revolution of Multimodal Large Language Models: A Survey](original_files/Caffagni_2024_The_Revolution_of_Multimodal_Large_Language_Models_A_Survey.pdf)

## 1 Introduction

**â€‹æ ¸å¿ƒä¸»é¢˜**â€‹ï¼šå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰é€šè¿‡èåˆè§†è§‰ä¸æ–‡æœ¬æ¨¡æ€ï¼Œå®ç°ç”Ÿæˆå¼å¯¹è¯ä¸æŒ‡ä»¤è·Ÿéšèƒ½åŠ›ã€‚

**â€‹ç ”ç©¶èƒŒæ™¯**â€‹ï¼šå—Transformeræ¶æ„å’ŒLLMsæˆåŠŸå¯å‘ï¼Œæ¨¡å‹ä»å•æ¨¡æ€æ‰©å±•åˆ°å¤šæ¨¡æ€ï¼ˆå¦‚GPT-4Vã€Geminiï¼‰ã€‚

**â€‹è®ºæ–‡è´¡çŒ®**â€‹ï¼šå…¨é¢åˆ†æè§†è§‰æ¨¡æ€MLLMsçš„æ¶æ„ã€è®­ç»ƒæ–¹æ³•ã€ä»»åŠ¡æ€§èƒ½åŠæŒ‘æˆ˜ï¼Œè¦†ç›–200+ç¯‡æ–‡çŒ®ã€‚

## 2 Empowering LLMs with Multimodal Capabilities

![alt text](pics/image4.png)

### 2.1 Preliminaries

#### Parameter-efficient fine-tuning (PEFT) 

When a pre-trained LLM needs to be adapted to a specific domain or application, PEFT schemes represent an important alternative to train the entire LLM, since these strategies **only introduce a few new parameters**.

å¸¸ç”¨æŠ€æœ¯åŒ…æ‹¬ **prompt tuning**ã€**LoRA** å’Œ **QLoRA**

>***Question***: 
**PEFTå’ŒFTä»¥åŠSFTæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ**

>***Answer*** (Deepseek):
**FTï¼ˆå…¨å‚æ•°å¾®è°ƒï¼‰**
â€‹å®šä¹‰â€‹ï¼šä¼ ç»Ÿå¾®è°ƒæ–¹æ³•ï¼Œæ›´æ–°é¢„è®­ç»ƒæ¨¡å‹çš„æ‰€æœ‰å‚æ•°
â€‹ç‰¹ç‚¹â€‹ï¼šéœ€è¦å¤§é‡è®¡ç®—èµ„æºå’Œå­˜å‚¨ç©ºé—´ï¼›é€‚åˆæ•°æ®å……è¶³ã€ä»»åŠ¡ç‰¹å®šçš„åœºæ™¯ï¼›åœ¨æ–‡æ¡£ä¸­ä½“ç°ä¸º"LLM fine-tuning"æ ‡è®°
**SFTï¼ˆç›‘ç£å¼å¾®è°ƒï¼‰**
â€‹å®šä¹‰â€‹ï¼šFTçš„ä¸€ç§ç‰¹æ®Šå½¢å¼ï¼Œä¸“æ³¨äºæŒ‡ä»¤è·Ÿéšèƒ½åŠ›çš„ä¼˜åŒ–
â€‹ç‰¹ç‚¹â€‹ï¼šä½¿ç”¨é«˜è´¨é‡çš„æŒ‡ä»¤-å›ç­”å¯¹è¿›è¡Œè®­ç»ƒï¼›ç›®æ ‡æ˜¯ä½¿æ¨¡å‹è¡Œä¸ºä¸äººç±»æœŸæœ›å¯¹é½ï¼›åœ¨MLLMsä¸­å‘å±•ä¸º"visual instruction-tuning"
**PEFTï¼ˆå‚æ•°é«˜æ•ˆå¾®è°ƒï¼‰**
å®šä¹‰â€‹ï¼šä»…è®­ç»ƒå°‘é‡æ–°å¢å‚æ•°ï¼Œä¿æŒé¢„è®­ç»ƒæƒé‡å†»ç»“
â€‹ç‰¹ç‚¹â€‹ï¼šæ˜¾è‘—é™ä½è®¡ç®—å’Œå­˜å‚¨éœ€æ±‚ï¼›æ”¯æŒå¿«é€Ÿé¢†åŸŸé€‚åº”ï¼›åœ¨MLLMsè®­ç»ƒä¸­è¢«å¹¿æ³›é‡‡ç”¨

#### Towards Multimodal LLMs

- **in-context learning**

- **visual instruction-tuning**

- **components**: 1. LLM backbone; 2. visual encoders; 3. vision-to-language adapter modules.

#### Pre-Training of Model Components

- **visual encoder**: is designed to provide LLMs with visual information and the most used ones are CLIP-based architectures (Radford et al., 2021; Wortsman et al., 2022) whose pretraining objective is the alignment between **CLIP (Contrastive Language-Image Pre-training)** embeddings, obtained thanks to a contrastive loss that aligns the correct image-text pairs.

- **LLMs**: primarily rely on the widely employed Transformer model, although the Mamba architecture (Gu and Dao, 2023) has also emerged in recent times.

![alt text](pics/image5.png)

### 2.2 Visual Encoder

The most often employed visual encoders are based on **pre-trained Vision Transformer (ViT)** models with a CLIP-based objective to exploit the inherent alignment of CLIP embeddings.

### 2.3 Vision-to-Language Adapters

Facilitate interoperability between the visual and textual domains.

- **Linear and MLP Projections**

- **Q-Former**

- **Additional Cross-Attention Layers**

### 2.4 Multimodal Training

- **Single-Stage Training**

- **Two-Stage Training**

- **Training Data**

## 3 Tackling Visual Tasks with MLLMs

### 3.1 Visual Grounding

![alt text](pics/image6.png)

- **Region-as-Text**: **directly** insert them into generated text as **a series of coordinates**, represented as numbers or as special tokens dedicated to location bins.

- **Embedding-as-Region**: read input regions through **region encoders** and provide the output regions as **embeddings** extracted from the last layer of the MLLM to a decoder.

- **Text-to-Grounding**: based on **open-vocabulary models** that accept textual categories as input.

### 3.2 Image Generation and Editing

![alt text](pics/image7.png)

- **Connecting MLLMs with Diffusion Models**: map the output embedding space of a **frozen LLM** to that of a **frozen diffusion model**.

- **End-to-End Pipelines**: 

### 3.3 Other Modalities and Applications

- **Video Understanding**

- **Any-Modality Models**

- **Domain-Specific MLLMs**

## 4 Conclusion and Future Directions

- **Multimodal Retrieval-Augmented Generation**

- **Correction of Hallucinations**

- **Prevent Harmful and Biased Generation**

- **Reducing Computational Load**